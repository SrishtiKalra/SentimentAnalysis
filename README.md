# Sentiment Analysis using BERT 

This project implements **Sentiment Analysis** using the **Bidirectional Encoder Representations from Transformers (BERT)** model. The goal is to analyze and classify sentiments from textual data (e.g., product reviews, social media comments) into categories such as **positive, negative, and neutral**.

## Features
-  **Pre-trained BERT Model** – Uses **Google’s BERT model** for state-of-the-art sentiment classification.
- **Fine-tuned Model** – Improves performance by fine-tuning on a sentiment dataset.
- **Text Classification** – Predicts sentiment labels based on textual input.
- **Hyperparameter Optimization** – Experiments with different learning rates, batch sizes, and custom layers to enhance accuracy.
- **Improved Accuracy** – Achieves better sentiment classification performance compared to traditional models.

## Tech Stack
- **Language & Libraries**: Python, TensorFlow, PyTorch, Hugging Face Transformers
- **Pre-trained Model**: BERT (Bidirectional Encoder Representations from Transformers)
- **Data Processing**: Pandas, NumPy, Scikit-learn, NLTK
- **Visualization**: Matplotlib, Seaborn

## Setup Instructions

### Prerequisites
Ensure you have the following installed:
- Python (>= 3.7)
- TensorFlow (>= 2.0) or PyTorch
- Transformers (Hugging Face)
- Scikit-learn
- Pandas & NumPy

### Installation & Running Locally
1. **Clone the Repository**
   ```sh
   git clone https://github.com/SrishtiKalra/SentimentAnalysis.git
   cd SentimentAnalysis
